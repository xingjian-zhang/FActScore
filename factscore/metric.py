import hashlib
import json
import os
from typing import Any, Dict, List

import numpy as np

from factscore.factscorer import FactScorer


class FActScore:
    """This is an extension of the original FActScore metric.

    The original FActScore is a metric for evaluating the factual precision of
    text generation given a trusted corpus [1]. This class extends the original
    FActScore metric to
    - accept a list of references as ground truth, instead of using RAG to
        retrieve evidence from the trusted corpus.
    - compute the precision, recall, and F1 score.

    1. [FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long
       Form Text Generation](https://arxiv.org/abs/2305.14251)

    """
    def __init__(self, cache_dir="./cache/factscore", log_dir="./logs/factscore2",strictness="moderate", name="plagiarism_200"):
        self.cache_dir = cache_dir
        self.log_dir = log_dir
        self.strictness = strictness
        self.name = name

    def _compute(self, predictions: List[str], references: List[str]) -> Dict[str, Any]:
        """Compute the precision of the atomic facts extracted from the
        predictions compared to the atomic facts in the references."""
        hash_key = hashlib.md5("\n".join(predictions + references).encode()).hexdigest()
        cache_dir = os.path.join(self.cache_dir, hash_key)
        fs = FactScorer(cache_dir=cache_dir, strictness=self.strictness)
        dummy_topics = [None] * len(predictions)
        scores = fs.get_score(dummy_topics, predictions, references, verbose=False, gamma=0)
        return scores

    def compute(self, predictions: List[str], references: List[str]) -> Dict[str, Any]:
        """Compute the precision, recall, and F1 score of the atomic facts
        extracted from predictions with respect to the atomic facts in the
        references.

        Args:
            predictions (List[str]): A list of natural language sentences that
                are generated by a language model.
            references (List[str]): A list of natural language sentences that
                are considered as ground truth.

        Returns:
            Dict[str, Any]: A dictionary containing the `precision`, `recall`, and
                `f1` score.
        """
        results =[]
        for i, (prediction, reference) in enumerate(zip(predictions, references)):
            prediction = [prediction]
            reference = [reference]
            # hash_key = hashlib.md5("\n".join(prediction + reference).encode()).hexdigest()
            # Compute precision of atomic facts in the predictions.
            precision_out = self._compute(prediction, reference)
            precision = float(np.mean(precision_out["score"]))
            # Compute recall of atomic facts in the references.
            # This is equivalent to computing the precision of atomic facts in the
            # references.
            recall_out = self._compute(reference, prediction)
            recall = float(np.mean(recall_out["score"]))
            # Compute the F1 score.
            if precision + recall == 0:
                f1 = 0.0
            else:
                f1 = 2 * precision * recall / (precision + recall)

            rst = {
                "case_id": i,
                "prediction": prediction,
                "reference": reference,
                "precision": precision_out,
                "recall": recall_out,
                "f1": f1
            }
            results.append(rst)
            # Dump the output to a json file.
            if self.log_dir is not None:
                os.makedirs(self.log_dir, exist_ok=True)
                log_path = os.path.join(self.log_dir, f"{self.strictness}_{self.name}" + ".jsonl")
                with open(log_path, "a") as f:
                    f.write(json.dumps(rst) + "\n")

        return results